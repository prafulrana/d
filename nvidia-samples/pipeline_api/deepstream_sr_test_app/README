*****************************************************************************
 * SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: LicenseRef-NvidiaProprietary
 *
 * NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
 * property and proprietary rights in and to this material, related
 * documentation and any modifications thereto. Any use, reproduction,
 * disclosure or distribution of this material and related documentation
 * without an express license agreement from NVIDIA CORPORATION or
 * its affiliates is strictly prohibited.
*****************************************************************************

===============================================================================
1. Purpose:
===============================================================================

This document describes the deepstream-sr-test application.

This sample demonstrates how to:
* Use smart-record feature for recording RTSP streams
* Configure and manage smart recording based on events
* Handle multiple RTSP streams with smart recording capabilities
* Implement event-based recording using Kafka messaging system

===============================================================================
2. Usage:
===============================================================================

Run the application with one or more RTSP URIs as arguments:

    $ python3 deepstream_sr_test.py rtsp://127.0.0.1/video1 [rtsp://127.0.0.1/video2 ...]

The application supports multiple RTSP streams and will automatically configure the pipeline
for each stream.

===============================================================================
3. Pipeline Components:
===============================================================================

The application implements the following pipeline components:
* nvurisrcbin: Source component for RTSP streams with smart recording enabled
* nvstreammux: Stream multiplexer for handling multiple sources
* nvinfer: Primary inference engine using config_infer_primary.yml
* nvmultistreamtiler: Tiler for displaying multiple streams
* nvosdbin: On-screen display component
* nv3dsink/nveglglessink: Display sink (automatically selected based on platform)

===============================================================================
4. Smart Recording Configuration:
===============================================================================

The smart recording feature is configured with the following parameters:
* Smart record cache: 20 seconds
* Recording directory: Current directory (.)
* Recording mode: 0 (default)
* Container format: 0 (default)

The application uses Kafka for event-based recording control:
* Kafka protocol library: /opt/nvidia/deepstream/deepstream/lib/libnvds_kafka_proto.so
* Kafka configuration: /opt/nvidia/deepstream/deepstream/sources/libs/kafka_protocol_adaptor/cfg_kafka.txt
* Kafka connection: localhost:9092
* Kafka topic: sr-test

===============================================================================
5. Important Notes:
===============================================================================

1. Smart Record feature only works with live RTSP streams
2. You need to configure MSGCONV_CONFIG_FILE for smart-record feature
3. A sample MSGCONV_CONFIG_FILE is available at:
   "/opt/nvidia/deepstream/deepstream/sources/apps/sample_apps/deepstream-test5/configs/dstest5_msgconv_sample_config.txt"
4. The smart record feature allows for:
   - Event-based recording through Kafka messaging
   - Configurable recording duration and cache
   - Multiple stream handling with individual control
   - Custom event triggers for recording
   - Automatic pipeline configuration for each stream

===============================================================================
6. How To Smart Record
===============================================================================

Follow these steps to use the smart record feature:

1. **Install a Message Broker**
   Download and install a message broker like Kafka or MQTT.

2. **Start the Message Broker Producer**
   Use the message broker producer script in the message broker directory to send smart record messages.

   **For Kafka example:**

   a) Download and extract Apache Kafka:
   ```bash
   wget https://downloads.apache.org/kafka/3.6.1/kafka_2.13-3.6.1.tgz
   tar -xzf kafka_2.13-3.6.1.tgz
   cd kafka_2.13-3.6.1
   ```

   b) Start Kafka server (in a separate terminal):
   ```bash
   bin/zookeeper-server-start.sh config/zookeeper.properties
   bin/kafka-server-start.sh config/server.properties
   ```

   c) Start the Kafka console producer:
   ```bash
   bin/kafka-console-producer.sh --topic test5-sr --bootstrap-server localhost:9092
   ```

   **Note:** Update the topic name (`test5-sr`) and server address (`localhost:9092`) to match the values configured in your application configuration file.

3. **Send Smart Record Command**
   Once the script starts, send the smart-record command in JSON format:

   ```json
   {
     "command": "start-recording",
     "start": "2025-07-15T07:35:00.051Z",
     "end": "2025-07-16T07:35:10.051Z",
     "sensor": {
       "id": "HWY_20_AND_LOCUST__EBA__4_11_2018_4_59_59_508_AM_UTC-07_00"
     }
   }
   ```

**Notes:**
- Update the UTC time according to your current time and date
- Sensor ID mapping is configured using the file:
  `/opt/nvidia/deepstream/deepstream/sources/apps/sample_apps/deepstream-test5/configs/dstest5_msgconv_sample_config.txt`
- Update the sensor ID accordingly based on your configuration